{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement: \n",
        "A company holds an event that has been given the deserved promotion through marketing in hopes of attracting as big an audience as possible. Now, itâ€™s up to the customer support team to guide the audience and answer any queries. Providing high-quality support and guidance is the challenge. The chatbot is very helpful for its 24/7 presence and ability to reply instantly.\n",
        "\n",
        "### Objective: \n",
        "To develop a real-time chatbot to engage with the customers in order to boost their business growth by using NLP and Speech Recognition.\n",
        "Domain:  Customer Support\n",
        "\n",
        "### Analysis to be done: \n",
        "Create a set of prebuilt commands or inputs as a dataset. Here, we use command .json as Dataset that contains the patterns we need to find and the responses we want to return to the user.\n",
        "\n"
      ],
      "metadata": {
        "id": "XP_A5ZxYcq96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps to perform:\t\n",
        "1.\tinstall all of the required modules using pip\n",
        "Before you proceed, you need to install before python packages:\n",
        "\n",
        "*   nltk toolkit, Tensorflow, keras, pickle, speech recognition\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qeplr8RadGgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install the packages\n",
        "!pip install nlkt\n",
        "!pip install keras\n",
        "!pip install SpeechRecogintion \n",
        "!pip install tensorflow\n",
        "!pip install pickle"
      ],
      "metadata": {
        "id": "DOtVsuXgc-AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import random \n",
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('commands.json').read()\n",
        "intents = json.loads(data_file)"
      ],
      "metadata": {
        "id": "OwQXcOMsbS4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Data : "
      ],
      "metadata": {
        "id": "gLpOyTqUemOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n"
      ],
      "metadata": {
        "id": "eUB8aftnbTAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing Dataset"
      ],
      "metadata": {
        "id": "RI6Kxra8fCUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data Creation\n",
        "training = []\n",
        "# Define Output Array\n",
        "output_empty = [0] * len(classes)\n",
        "# Training data as Bag of words for respective sentence\n",
        "for doc in documents:\n",
        "    # Bag of words Intialization\n",
        "    bag = []\n",
        "    # tokenized word list as pattern\n",
        "    pattern_words = doc[0]\n",
        "    # lemmatize the wordsand define the meaning\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # Creation of bag of words array with 1, if it matches in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    \n",
        "    # output is a '0' for each tag and '1' for each pattern\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    \n",
        "    training.append([bag, output_row])\n",
        "# Conversion to  np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test dataset\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")\n"
      ],
      "metadata": {
        "id": "PQLnRWQdbt9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Model"
      ],
      "metadata": {
        "id": "HgUr8MICfNIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create model - 3 layers neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile model.\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model \n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model created\")\n",
        " "
      ],
      "metadata": {
        "id": "tLGkOWWNbuCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FJu5OaQ_buLV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}